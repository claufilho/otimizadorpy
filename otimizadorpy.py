import numpy as np
import sympy as sp
from scipy.linalg import solve, LinAlgError
import re

def otimizar(funcao_str, variaveis_str, x0, metodo='newton', tol=1e-6, max_iter=100):
    # Cria√ß√£o das vari√°veis
    var_names = variaveis_str.replace(' ', '').split(',')
    vars_sym = sp.symbols(var_names)
    f = sp.sympify(funcao_str)

    # Gradiente e Hessiana
    grad_f = sp.Matrix([sp.diff(f, var) for var in vars_sym])
    hess_f = sp.hessian(f, vars_sym)

    # Fun√ß√µes Num√©ricas
    grad_f_func = sp.lambdify(vars_sym, grad_f, 'numpy')
    hess_f_func = sp.lambdify(vars_sym, hess_f, 'numpy')
    f_func = sp.lambdify(vars_sym, f, 'numpy')

    # In√≠cio do m√©todo
    x_k = np.array(x0, dtype=float)
    I = np.eye(len(x_k))

    for k in range(max_iter):
        # C√°lculo do gradiente e Hessiana na posi√ß√£o atual
        grad = np.array(grad_f_func(*x_k)).astype(np.float64).reshape(-1)
        hess = np.array(hess_f_func(*x_k), dtype=float)

        # Verifica√ß√£o do crit√©rio de parada baseado na norma do gradiente
        grad_norm = np.linalg.norm(grad)
        if grad_norm < tol:
            print(f'\n‚úÖ Converg√™ncia atingida em {k} itera√ß√µes. Norma do gradiente: {grad_norm:.6e}')
            return x_k, f_func(*x_k)

        # Verificando o crit√©rio de converg√™ncia para a fun√ß√£o objetivo
        f_value = f_func(*x_k)
        if abs(f_value) < tol:  # Se a fun√ß√£o atingir um valor pequeno, pode ser considerado convergido
            print(f'\n‚úÖ Converg√™ncia atingida em {k} itera√ß√µes. Fun√ß√£o objetivo: {f_value:.6e}')
            return x_k, f_value

        # Newton Modificado (caso a Hessiana n√£o seja bem condicionada)
        if metodo == 'modificado':
            eigvals = np.linalg.eigvalsh(hess)
            min_eig = np.min(eigvals)
            if min_eig <= 0:
                tau = 1e-3 - min_eig  # Ajuste da regulariza√ß√£o para melhorar a condi√ß√£o da Hessiana
                hess += tau * I

        try:
            # C√°lculo do passo de Newton
            p_k = solve(hess, -grad)
        except LinAlgError:
            print('‚ùå Hessiana singular. M√©todo falhou.')
            return x_k, None

        # Atualiza√ß√£o da posi√ß√£o
        x_k = x_k + p_k

        # Exibindo o progresso
        if k % 10 == 0:  # A cada 10 itera√ß√µes, mostramos o progresso
            print(f'Itera√ß√£o {k}: Norma do gradiente = {grad_norm:.6e}, Fun√ß√£o objetivo = {f_value:.6e}')

    print('\n‚ö†Ô∏è M√°ximo de itera√ß√µes atingido.')
    return x_k, f_func(*x_k)

# =============================
#      Entrada do usu√°rio
# =============================
if name == "_main": 
    print("üîß Otimiza√ß√£o com m√©todo de Newton / Newton Modificado")
    funcao_str = input("Digite a fun√ß√£o a ser minimizada (ex: 100*(x2 - x1*2)2 + (1 - x1)*2): ")
    variaveis_str = input("Digite as vari√°veis separadas por v√≠rgula (ex: x1, x2): ")

    x0_str = input("Digite o ponto inicial como lista (ex: -1.0, 2.0): ")
    x0 = [float(num) for num in re.findall(r'-?\d+\.?\d*', x0_str)]

    metodo = input("Escolha o m√©todo ('newton' ou 'modificado'): ").strip().lower()

    # Execu√ß√£o
    xmin, fmin = otimizar(funcao_str, variaveis_str, x0, metodo)

    print("\nüìç M√≠nimo encontrado:")
    print("x* =", xmin)
    print("f(x*) =", fmin)
